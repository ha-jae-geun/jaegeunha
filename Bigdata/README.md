# 머신러닝, 딥러닝 차이
* 머신러닝: 기계가 자발적으로 판단하는 것이 아니다
* 머신러닝은 AI를 달성하기 위한 방법
  * Supervised: * 인간이 준 레이블을 기준으로 학습
  * Unsupervised: * 인간은 데이터를 분류하지 않는다. 기계로 하여금 엄청난 프로세싱 파워와 데이터를 토대로 학습시키는 것임
* 딥러닝: 머신러닝을 달성하기 위한 방법
  * 뇌처럼 작용하는 알고리즘이 있는 Neural Network를 사용한다는 차이; 
* AI를 위한 가장 유명한 프레임워크: 텐서플로우 혹은 BrainJS

## 범위
* 딥러닝 < 머신러닝 < AI

## 머신러닝 종류
* Supervised: 지도 학습(Supervised Learning)은 데이터에 대한 레이블(Label)-명시적인 정답-이 주어진 상태에서 컴퓨터를 학습시키는 방법이다.
  * 범주형, 연속형
  * 인간이 준 레이블을 기준으로 학습
* Unsupervised: 즉, (데이터(data)) 형태로 학습을 진행하는 방법이다. 예를 들어서, 아래와 데이터가 무작위로 분포되어 있을때, 이 데이터를 비슷한 특성을 가진 세가지 부류로 묶는 클러스터링(Clustering) 알고리즘이 있다.  데이터의 숨겨진(Hidden) 특징(Feature)이나 구조를 발견하는데 사용된다.
  * 군집화
  * 인간은 데이터를 분류하지 않는다. 기계로 하여금 엄청난 프로세싱 파워와 데이터를 토대로 학습시키는 것임
* 강화학습: 앞서 살펴본 알고리즘들이 데이터(data)가 주어진 정적인 상태(static environment)에서 학습을 진행하였다면, 강화 학습(Reinforcement Learning)은 에이전트가 주어진 환경(state)에 대해 어떤 행동(action)을 취하고 이로부터 어떤 보상(reward)을 얻으면서 학습을 진행한다. 이때, 에이전트는 보상(reward)을 최대화(maximize)하도록 학습이 진행된다. 즉, 강화학습은 일종의 동적인 상태(dynamic environment)에서 데이터를 수집하는 과정까지 포함되어 있는 알고리즘이다.

# 빅데이터
* 연속형 변수(X): 연속형 반응변수(Y): 선형회귀보델
* 연속형 변수(X): 범주변수(Y): 로지스틱 모형
* 범주 변수(X): 연속형 반응변수(Y): 분산분석
* 범주 변수(X):  범주변수(Y): 분할표 분석(하나의 X) 로지스틱 모형 또는 로그 선형 모형(여러개의 Xs)
* 연속형 변수(X) + 범주 변수(X): 연속형 반응변수(Y): 공분산분석 선형회귀(가변수) 모형
* 연속형 변수(X) + 범주 변수(X): 연속형 범주변수(Y): 로지스틱 모형

## 정확도
* 의사결정: 해석 <> 신경망: 정확성



# 귀무가설, 대립가설
* 예를 들어 회귀분석의 경우 귀무가설은 "설명변수(x)는 반응변수(y)에 영향을 주지 않는다." 이고 대립가설은 "설명변수(x)는 반응변수(y)에 영향을 준다." 이다. 그래서 편하게 기억하자면,  귀무가설은 차이가 없다, 영향력이 없다, 연관성이 없다, 효과가 없다. 대립가설은 차이가 있다, 영향력이 있다, 연관성이 있다, 효과가 있다. 라고 기억하는 게 편하다.
* 대립가설(Alternative Hypothesis)은 통계학에서 귀무가설에 대립하여 '모집단에서 독립변수와 결과변수 간에 관련이 있다'라고 기술하는 명제를 말한다. 연구가설 혹은 유지가설이라고도 하며 어떤 가능성에 대해 확률적인 가설검정을 할 때 귀무가설과 함께 사용된다. 이 가설은 귀무가설처럼 검정을 직접 수행하기는 불가능하며 귀무가설을 기각함으로써 받아들여지는 반증의 과정을 거쳐 받아들여질 수 있다. 대립가설은 양측대립가설과 단측대립가설이 있다.

# 파이썬 
- 클래스 방식은 컴파일러 -> pyc파일 -> 인터프리터; 
- 함수: 인터프리터 방식

# 빅데이터 과정
## 파이썬 수집 -> 기본 파이썬
- 웹 크롤링 -> 스크래핑
- 데이터 세트

## 파이썬 분석
- 판다스

## 머신러닝
* knn 알고리즘, SVM, 의사결정트리, 나이브베이즈, 신경망 알고리즘
* 크로스 벨리데이스, 그리드 서치, 형태소 분석

## 딥러닝
* 신경망: 텐서 플로우, 케라스


# '웹 크롤링', '스크래핑'

# 크롤링
* (2) 크롤링 크롤링은 웹 사이트를 정기적으로 돌며 정보를 추출하는 기술이다. 크롤링하는 프로그램을 크롤러 또는 스파이더라고 한다. 
* 크롤링으로 웹 정보를 추출할 때는 반드시 하나의 특정 웹 페이지의 URL 주소를 명시해주어야 한다. 가장 먼저 방문하는 URL은 중요한 역할을 하는데 뒤이어 크롤링 되어야할 웹 페이지의 URL은 웹 페이지가 링크하고 있는 웹 페이지를 순차적으로 방문한다. 

# 스크레핑
* (3) 스크레이핑 스크레이핑은 웹 사이트에 있는 특정 정보를 추출하는 기술이다. 스크레이핑은 웹에서 자료를 추출 하는 것 뿐만 아니라 구조도 분석한다. 스크레이핑은 로그인해서 필요한 웹 페이지에 접근 하는 기술이 필요하다. 
* 시스템이나 웹사이트에 있는 정보(데이터) 중에서 필요한 정보를 추출 및 가공하여 제공하는 소프트웨어 기술입니다. 이는 고객이 한 번만 자신의 정보 제공을 인증하게 되면 컴퓨터가 대신하여 필요한 고객 정보들을 자동으로 추출하여 제공하는 것을 말합니다. 

## 뱅크 샐러드
* 뱅크샐러드라는 하나의 애플리케이션을 통해 고객은 자신의 카드사 및 은행, 보험정보들을 한번에 조회할 수 있습니다.

## 스크래핑 종류
* 금융위는 또 서버에 저장하고 관리해 문제가 있다고 거론하는데 이는 서버형 스크래핑이다. 이를 이용하는 업체는 일부인데, 기술력을 보유한 스크래핑 개발업체는 이 방식을 지양하고 있다. 서버형 보다는 클라이언트형 스크래핑을 활용한다. 스마트폰 보급률이 높기 때문이다. 스마트폰에 저장된 개인 인증서를 스크래핑 엔진이 있는 업체에게 곧바로 가기 때문에 제3자가 개인정보를 보거나 유출하기 힘들다. 또 최근 정보 유출 사고 사례 중 스크래핑에 의한 경우는 없었다."
* "API는 무조건 안정적이며 스크래핑은 안정적이지 않다는 비교는 하기 어렵다. 스크래핑은 지속적으로 보안성을 높이는 기술을 개발하고 업그레이드해왔다. 다만 스크래핑은 대상 사이트의 보안정책을 준수하지만, 통신프로토콜을 적용한 API방식은 보안성이 뛰어난 것은 사실이다.


# 은행
## OTP
* OTP(One Time Password)발생기는 전자금융 거래에서 사용되는 일회용 비밀번호 생성 기기로 1분마다 새로운 비밀번호가 생성되어 해킹이나 외부노출의 위험으로부터 안전하게 서비스를 이용하실 수 있습니다.
* 보안카드 불편성 해결(앞자리 뒷자리, 비밀번호 3개 입력),  OTP는 6자리 비밀번호 그대로 입력
* 1분마다 새롱누 6자리 비밀번호 생성

## OTP 단점
* 하지만, 전원문제로 소진이 되면 보안카드 보다 훨씬 더 불편함을 발생할 수 있는데 이를 감추었다고 할 수 있다. 이 같은 상황은 국내 은행들이 발행하는 OTP 비밀번호 발생기는 3V 배터리가 내장되어 밀봉된 형태로, 배터리가 소진하면 재사용이 불가하고, 은행으로부터 새로운 기기를 수령해야 하며, 결과적으로 OTP도 새롭게 등록해야 합니다.
* 반면, 영국은행에서 지급되는 OTP는 국내은행과는 다른 형태의 기기를 제공한다. 사실상 우리의 공인인증서 역할을 함께 한다고 할 수 있다. 그 차이점은 단말기에 '사용자 확인(Identify), 서명(sign), Response등 복합적인 기능을 한꺼번에 처리가 가능하다. 그리고, 우리 개인용 OTP와는 달리, 키패드도 내장되어 있어서, 필요한 번호를 입력함으로써 보안성을 높였다고 할 수 있다.


# 회기분석
* 회귀분석은 독립변수와 종속변수 사이의 구체적인 함수식을 찾아내고, 독립변수로부 터 종속변수를 예측하는 데 그 목적이 있다.
* 선형회귀, 다중회귀분석

## 독립변수
- 독립변수는 연구자가 의도적으로 변화시키는 변수를 말합니다.
- 독립변수는 영어로 Independent variable입니다. 말 그대로 독립적인 변수입니다. 통계에서 독립적이라는 말은 다른 변수에 영향을 받지 않는다는 뜻입니다. 따라서 독립변수는 다른 변수에 영향을 받지 않습니다. 오히려 종속 변수에 영향을 주는 변수 입니다. 
* 회귀분석에서 다른 변수에 영향을 주는 원인에 해당하는 변수를 독립변수(independent variable) 또는 설명변수(explanatory variable)라고 하며

## 종속변수
- 영향을 받는 결과에 해당하는 변 수를 종속변수(dependent variable) 또는 반응변수(response variable)라고 한다. 
- 종속변수는 연구자가 독립변수의 변화에 따라 어떻게 변하는지 알고 싶어하는 변수를 말합니다.
- 그렇다면 종속변수는 무엇일까요? 종속변수는 영어로 Dependent variable 입니다. 말 그대로 종속적인 또는 의존적인 변수입니다. 즉 독립변수에 영향을 받아서 변화하는 변수를 종속변수라고 생각하시면 됩니다. 
- 조금 어렵나요? 더욱 쉽게 설명하면 독립변수는 연구자가 마음대로 조정할 수 있는 변수입니다. 왜 일까요? 바로 독립변수가 어떻게 변화하느냐에 따라 종속변수가 어떻게 변화하는지 보고 싶기 때문이죠. 이 때문에 독립변수는 원인변수 (Explanatory variable), 예측 변수 (Predictor variable)라고 부르기도 합니다. 반면에 종속 변수는 연구자가 알고 싶어하는 변수입니다. 연구자의 목표는 독립변수를 조정하여 변화시킬 때 종속변수가 어떻게 변화하는지 알아내는 것 입니다. 이 때문에 종속변수를 반응 변수 (Response variable), 결과 변수 (Outcome variable)이라고 부르기도 합니다. 

## MAE
* Mean absolute error
* -통계에서 평균 절대 오차는 두 연속 변수의 차이를 측정 한 것입니다

# 판다스, 맷플롯립
## 판다스
1. 시리즈(Series): 시리즈 클래스는 1차원 배열의 값(values)에 각 값에 대응되는 인덱스(index)를 부여할 수 있는 구조를 갖고 있습니다.
2. 데이터프레임(DataFrame): 은 2차원 리스트를 매개변수로 전달합니다. 2차원이므로 행방향 인덱스(index)와 열방향 인덱스(column)가 존재합니다. 즉, 행과 열을 가지는 자료구조입니다. 시리즈가 인덱스(index)와 값(values)으로 구성된다면, 데이터프레임은 열(columns)까지 추가되어 열(columns), 인덱스(index), 값(values)으로 구성됩니다.
3. 패널(Panel)

## 넘파이
* Numpy(보통 "넘파이"라고 발음.)는 수치 데이터를 다루는 Python 패키지입니다. Numpy의 핵심이라고 불리는 다차원 행렬 자료구조인 ndarray를 통해 벡터 및 행렬을 사용하는 선형 대수 계산에서 주로 사용됩니다. Numpy는 편의성뿐만 아니라, 속도면에서도 순수 파이썬에 비해 압도적으로 빠르다는 장점이 있습니다.

## 사이킷런
*  머신러닝 학습용 패키지다. 대부분의 머신러닝 모형을 제공하므로 파이썬으로 머신러닝을 공부하는 데 최적의 학습 도구다.
* https://book.coalastudy.com/data-science-lv1/week3/stage3

### Training set
* 모델의 학습에 사용되는 데이터

### Validation set
* 모델 제작 과정 중, 학습된 모델의 성능을 측정하기 위한 데이터
### Test set 모델의 최종 성능을 평가하기 위한 데이터
* 모델의 최종 성능을 평가하기 위한 데이터


## 맷플롯립
* Matplotlib는 데이터를 차트(chart)나 플롯(plot)으로 시각화(visulaization)하는 패키지입니다. 데이터 분석에서 Matplotlib은 데이터 분석 이전에 데이터 이해를 위한 시각화나, 데이터 분석 후에 결과를 시각화하기 위해서 사용됩니다.

## Seaborn¶
* Seaborn("시본"이라고 읽는다) 패키지는 Matplotlib 패키지에서 지원하지 않는 고급 통계 차트를 그리는 통계용 시각화 기능을 제공한다.



# 신경망 알고리즘
* Artificial neural network (ANN)는 딥 러닝의 가장 핵심적인 기술로써, 신경 세포인 neuron을 추상화한 artificial neuron으로 구성된 네트워크이다. ANN은 일반적으로 어떠한 형태의 function이든 근사할 수 있는 universal function approximator로도 알려져 있다. 이 글에서는 ANN을 구성하는 가장 작은 요소인 artificial neuron부터 multi-layer ANN과 이를 학습시키기 위한 algorithm에 대해 서술한다.
* 일반적으로 사용되는 기본적인 인공신경망 알고리즘인 다층인공신경망(multi-layer neural network)의 경우 아래 그림과 같이 입력층(input layer), 은닉층(hidden layer), 그리고 출력층(output layer),이렇게 세가지 층으로 구분이 됩니다. 그리고 각 층들은 노드들로 구성되어 있습니다. 아래 그림의 예에서는 입력층은 4개의 노드, 은닉층은 3개의 노드, 그리고 출력층은 1개의 노드를 가지고 있습니다.
* 입력층은 예측값(출력변수)을 도출하기 위한 예측변수(입력변수)의 값들을 입력하는 역할을 합니다. 만약 n개의 입력 값들이 있다면 입력층은 n개의 노드를 가지게 됩니다. 은닉층은 모든 입력노드부터 입력값을 받아 가중합을 계산하고, 이 값을 전이함수에 적용하여 출력층에 전달하게 됩니다. 각 입력노드와 은닉노드들은 모두 가중치를 가지는 망으로 연결되어 있으며 은닉노드와 출력노드도 마찬가지로 연결되어 있습니다.
* 이 가중치는 연결강도로 표현되며 랜덤으로 초기에 주어졌다가 예측 값을 가장 잘 맞추는 값으로 조정되게 됩니다. 전이함수는 비선형함수를 사용하게 되며, 이러한 전이함수를 통하여 출력층에 예측 값이 전달되기 때문에 인공신경망이 비선형 모델로서 역할을 할 수 있게 됩니다.
* https://blog.lgcns.com/1359

## 의사결정트리
* 의사결정나무는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내며, 그 모양이 ‘나무’와 같다고 해서 의사결정나무라 불립니다. 질문을 던져서 대상을 좁혀나가는 ‘스무고개’ 놀이와 비슷한 개념입니다. 한번 예를 들어볼까요?
* 의사결정나무는 분류(classification)와 회귀(regression) 모두 가능합니다. 범주나 연속형 수치 모두 예측할 수 있다는 말입니다. 의사결정나무의 범주예측, 즉 분류 과정은 이렇습니다. 새로운 데이터가 특정 terminal node에 속한다는 정보를 확인한 뒤 해당 terminal node에서 가장 빈도가 높은 범주에 새로운 데이터를 분류하게 됩니다. 운동경기 예시를 기준으로 말씀드리면 날씨는 맑은데 습도가 70을 넘는 날은 경기가 열리지 않을 거라고 예측합니다.


# 텐서플로우
```python
# 랜덤 가중치
```python
import tensorflow as tf

import numpy as np

from pandas.io.parsers import read_csv

import os

# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

model = tf.global_variables_initializer()
#
data = read_csv('서울_서울소매_2010_2017(2)_03_2_강수.csv', sep=',', engine='python')


# data = read_csv('price data.csv', sep=',')

xy = np.array(data, dtype=np.float32)



# 4개의 변인을 입력을 받습니다.

x_data = xy[:, 0:-1]

# print(x_data)


# 가격 값을 입력 받습니다.

y_data = xy[:, [-1]]
# print(y_data)

# 가중치
# w_data = [-12.6495, 8.4675, -146.2447, -702.2301, -9.6260]
# print(w_data)

# 플레이스 홀더를 설정합니다.

X = tf.placeholder(tf.float32, shape=[None, 4])
# print(X)

Y = tf.placeholder(tf.float32, shape=[None, 1])
# print(Y)

W = tf.Variable(tf.random_normal([4, 1]), name="weight")

# W = tf.Variable(tf.random_normal([8, 1]), name="weight")
# print(W)

b = tf.Variable(tf.random_normal([1]), name="bias")
# b = tf.Variable(tf.random_normal([1]), name="bias")
# print(b)


# 가설을 설정합니다.

hypothesis = tf.matmul(X, W) + b



# 비용 함수를 설정합니다.

cost = tf.reduce_mean(tf.square(hypothesis - Y))



# 최적화 함수를 설정합니다.

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001)
# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000005)
# optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate=0.0000005)

train = optimizer.minimize(cost)



# 세션을 생성합니다.

sess = tf.Session()



# 글로벌 변수를 초기화합니다.

sess.run(tf.global_variables_initializer())



# 학습을 수행합니다.
for step in range(500000):

    cost_, hypo_, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})

    if step % 1000 == 0:

        print("#", step, " 손실 비용: ", cost_)

        print("- 배추 가격: ", hypo_[0])



# 학습된 모델을 저장합니다.

saver = tf.train.Saver()

save_path = saver.save(sess, "./saved.cpkt")

print('학습된 모델을 저장했습니다.')



```

```
